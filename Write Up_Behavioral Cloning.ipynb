{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/ncondo/CarND-Behavioral-Cloning\n",
    "* https://medium.com/@ksakmann/behavioral-cloning-make-a-car-drive-like-yourself-dc6021152713\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning Project\n",
    "\n",
    "## Udacity Self Driving Nanodegree - CarND-Term1\n",
    "\n",
    "#### JW / 12.8.2017\n",
    "\n",
    "---\n",
    "\n",
    "The goal of this project is to train a deep neural network drive the car autonomously around a simulated  around a test track in Udacity's driving simulator. The trained model try to predict right steering angles. The network is trained on from a video stream that was recorded while a human was steering the car. The CNN thus clones the human driving behavior.\n",
    "\n",
    "**The goals / steps of this project are the following:**\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "Please see the [rubric points](https://review.udacity.com/#!/rubrics/432/view) for this project.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/placeholder.png \"Model Visualization\"\n",
    "[image2]: ./examples/placeholder.png \"Grayscaling\"\n",
    "[image3]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image4]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image5]: ./examples/placeholder_small.png \"Recovery Image\"\n",
    "[image6]: ./examples/placeholder_small.png \"Normal Image\"\n",
    "[image7]: ./examples/placeholder_small.png \"Flipped Image\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements and Installations\n",
    "\n",
    "\n",
    "### Paths \n",
    "\n",
    "Jupyter notebook and model.h5 files\n",
    "cd /Users/jussi/Dropbox/Jussi/CarND-Term1-Starter-Kit/CarND-Behavioral-Cloning-P3-master\n",
    "Training Data \n",
    "~/users/Jussi/Desktop/CarND/data\n",
    "\n",
    "### Installations\n",
    "\n",
    "1. Install Miniconda with Python 3.6 and Udacity's carnd-term1 conda environment\n",
    "https://github.com/udacity/CarND-Term1-Starter-Kit/blob/master/doc/configure_via_anaconda.md\n",
    "  * CarND-Term1-Starter kit\n",
    "  git clone https://github.com/udacity/CarND-Term1-Starter-Kit.git\n",
    "  - Create carnd-term1 Environment: conda env create -f environment.yml\n",
    "2. Behavioral environment\n",
    "  * git clone https://github.com/ncondo/CarND-Behavioral-Cloning\n",
    "  \n",
    "  \n",
    "cd CarND-Term1-Starter-Kit\n",
    "3. Download Udacity's sample data and save it in the same directory as model.py\n",
    "  * Dataset https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip\n",
    "4. Download Udacity's driving simulator\n",
    "  * https://github.com/udacity/self-driving-car-sim\n",
    "\n",
    "\n",
    "### Simulator \n",
    "\n",
    "#### Collecting Data\n",
    "- Open simulator in training mode\n",
    "- Run view laps clockwise and counterclockwise\n",
    "- record some recovery shots (back to road)\n",
    "\n",
    "#### Testing your model\n",
    "- open another shell in that directory where is your model.h5 and type\n",
    "- python drive.py model.h5\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Evironment\n",
    "You need to create an environment for this projects:\n",
    "- conda env create -f environment.yml\n",
    "\n",
    "After the environment is created, it needs to be activated with the command:\n",
    "\n",
    "If need check environments \n",
    "* conda info --enf\n",
    "Activate environment\n",
    "* source activate carnd-term1\n",
    "Open Jupyter notebook\n",
    "* jupyter notebook\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "To train the model, first make a directory ./data/mydata, drive the car in training mode around the track and save the data to this directory. The model is then trained by typing\n",
    "\n",
    "- python model.py\n",
    "\n",
    "----\n",
    "To run the Visualizations you need [Graphviz][http://www.graphviz.org/]\n",
    "\n",
    "----\n",
    "\n",
    "### Files and Usage\n",
    "1. model.py \n",
    "  * Contains all code for reading in training data and training a model to predict steering angles.\n",
    "2. drive.py \n",
    "  * for driving the car in autonomous mode\n",
    "  * Contains code to serve predictions from a trained model to Udacity's simulator for autonomous driving.\n",
    "3. model.h5 \n",
    "  * containing a trained convolution neural network\n",
    "  * containing model weights\n",
    "4. writeup_report.md\n",
    "  * explain the structure of my network and training approach\n",
    "  \n",
    "The rest of this writeup.md provides details about the model used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. Solution Design Approach\n",
    "\n",
    "- CarND basic examble\n",
    "- nVidia\n",
    "- Comma.ai (https://github.com/commaai/research/blob/master/train_steering_model.py)\n",
    "- Data→VGG16 CNN→Custom Classifier. The general approach is\n",
    "based on the NVIDIA paper. (Joel Yarde) tarkista onko huru-ukko\n",
    "\n",
    "The overall strategy for deriving a model architecture was to ...\n",
    "\n",
    "My first step was to use a convolution neural network model similar to the ... I thought this model might be appropriate because ...\n",
    "\n",
    "---\n",
    "My first step was to try the LeNet](http://yann.lecun.com/exdb/lenet/) model with three epochs and the training data provided by Udacity.\n",
    "\n",
    "LeNet: \n",
    "![alt text](lenet.png \"LeNet\")\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "The second step was to use a more powerfull model: nVidia Autonomous Car Group The only modification was to add a new layer at the end to have a single output as it was required.\n",
    "\n",
    "---\n",
    "In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting. \n",
    "\n",
    "To combat the overfitting, I modified the model so that ...\n",
    "\n",
    "Then I ... \n",
    "\n",
    "The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track... to improve the driving behavior in these cases, I ....\n",
    "\n",
    "At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.\n",
    "\n",
    "#### 2. Final Model Architecture\n",
    "\n",
    "The final model architecture (model.py lines 18-24) consisted of a convolution neural network with the following layers and layer sizes ...\n",
    "\n",
    "Here is a visualization of the architecture (note: visualizing the architecture is optional according to the project rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "nVidia model: \n",
    "![alt text](examples/nvidia_cnn.png)\n",
    "\n",
    "-Käytä osia tästä:\n",
    "\n",
    "CNNs architectures have been successfully used to predict the steering angle of the simulator. Among these are the CNN architecture of NVIDIA or the comma.ai architecture which were used successfully, e.g. this submission. In this article Vivek Yadav provided a solution to this steering problem based on the judicious use of data augmentation. This submission draws on the insights obtained there but differs in the network architecture and augmentation techniques.\n",
    "\n",
    "In all of the above architectures a single variable -- the current steering angle -- is predicted as a real valued number. The problem is thus not a classification but a regression task. We will build a similar architecture that predicts a single real valued number, but it would be interesting to see how a discretized version performs.\n",
    "\n",
    "For the network architecture we draw on a CNN that evolved from a previous submission for classfying traffic signs with high (97.8%) accuracy. However, we included some crucial changes.\n",
    "\n",
    "The network starts with a preprocessing layer that takes in images of shape 64x64x3. Each image gets normalized to the range [-1,1] otherwise no preprocessing is performed. Following the input layer are 4 convolutional layers. ReLU activations are used throughout the whole network. The first two convolutional layers employ kernels of size k=(8,8) with a stride of s=(4,4) and 32 and 64 channels, respectively. The next convolutional layer uses k=(4,4) kernels, a stride of s=(2,2) and 128 channels. In the last convolutional layer we use k=(2,2), a stride s=(1,1) and again 128 channels. Following the convolutional layers are two fully connected layers with ReLU activations as well as dropout regularization right before the layers. The final layer is a single neuron that provides the predicted steering angle. We explicitly avoided the use of pooling layers because pooling layers apart from down sampling also provide (some) shift invariance, which is desirable for classification tasks, but is counterproductive for keeping a car centered on the road (note: the comma.ai architecture does not use pooling either).\n",
    "\n",
    "-Käytä tästä osa\n",
    "\n",
    "---\n",
    "Model architechture\n",
    "\n",
    "| Layer         | Output Shape           | Param no  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Lamda_1      | None, 64, 64, 3) | 0 |\n",
    "| convolution2d    | (None, 16, 16, 32)     |  636326|\n",
    "| activation_1 (Activation) | (None, 16, 16, 32)    |   0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creation of the Training Set & Training Process\n",
    "\n",
    "#### Running the Simulation\n",
    "\n",
    "To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "#### Data Collection\n",
    "\n",
    "I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to .... These images show what a recovery looks like starting from ... :\n",
    "\n",
    "![alt text][image3]\n",
    "![alt text][image4]\n",
    "![alt text][image5]\n",
    "\n",
    "Then I repeated this process on track two in order to get more data points.\n",
    "\n",
    "#### Data Visualization\n",
    "\n",
    "#### Training the Network\n",
    "\n",
    "\n",
    "\n",
    "- käytä tästä osia-\n",
    "\n",
    "All computations were run on an Ubuntu 16.04 system with an Intel i7 processor and an NVIDIA GTX 1080.\n",
    "Due to the problems with generating the important recovery events manually we decided on an augmentation strategy. The raw training data was gathered by driving the car as smoothly as possible right in the middle of the road for 3-4 laps in one direction. We simulated recovery events by transforming (shifts, shears, crops, brightness, flips) the recorded images using library functions from OpenCV with corresponding steering angle changes. The final training images are then generated in batches of 200 on the fly with 20000 images per epoch. A python generator creates new training batches by applying the aforementioned transformations with accordingly corrected steering angles. The operations performed are\n",
    "\n",
    "A random training example is chosen\n",
    "The camera (left,right,center) is chosen randomly\n",
    "Random shear: the image is sheared horizontally to simulate a bending road\n",
    "Random crop: we randomly crop a frame out of the image to simulate the car being offset from the middle of the road (also downsampling the image to 64x64x3 is done in this step)\n",
    "Random flip: to make sure left and right turns occur just as frequently\n",
    "Random brightness: to simulate differnt lighting conditions\n",
    "In steps 1-4 the steering angle is adjusted to account for the change of the image. The chaining of these operations leads to a practically infinite number of different training examples.\n",
    "\n",
    "The steering angle changes corresponding to each of the above operations was determined manually by investigating the result of each transformation and using trigonometry. For example the angle correction corresponding to a horizontal shearing transformation should be proportional to shearing angle.\n",
    "\n",
    "- käytä tästä osia-\n",
    "\n",
    "\n",
    "#### Data Preprosessing\n",
    "\n",
    "#### Data Augementation\n",
    "\n",
    "To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:\n",
    "\n",
    "![alt text][image6]\n",
    "![alt text][image7]\n",
    "\n",
    "Etc ....\n",
    "\n",
    "\n",
    "After the collection process, I had X number of data points. I then preprocessed this data by ...\n",
    "\n",
    "---\n",
    "\n",
    "Flipping Images And Steering Measurements\n",
    "A effective technique for helping with the left turn bias involves flipping images and taking the opposite sign of the steering measurement. For example: import numpy as np image_flipped = np.fliplr(image) measurement_flipped = -measurement The cv2 library also has similar functionality with the flip method.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validating the Network (+Epochs)\n",
    "\n",
    "I finally randomly shuffled the data set and put Y% of the data into a validation set. \n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary.\n",
    "\n",
    "---\n",
    "Validating the Data\n",
    "\n",
    "The validation set should contain image and steering data that was not used for training. A rule of thumb could be to use 80% of your data for training and 20% for validation or 70% and 30%. Be sure to randomly shuffle the data before splitting into training and validation sets.\n",
    "\n",
    "---\n",
    "In order to validate your network, you'll want to compare model performance on the training set and a validation set. The validation set should contain image and steering data that was not used for training. A rule of thumb could be to use 80% of your data for training and 20% for validation or 70% and 30%. Be sure to randomly shuffle the data before splitting into training and validation sets.\n",
    "If model predictions are poor on both the training and validation set (for example, mean squared error is high on both), then this is evidence of underfitting. Possible solutions could be to\n",
    "increase the number of epochs\n",
    "add more convolutions to the network.\n",
    "When the model predicts well on the training set but poorly on the validation set (for example, low mean squared error for training set, high mean squared error for validation set), this is evidence of overfitting. If the model is overfitting, a few ideas could be to\n",
    "use dropout or pooling layers\n",
    "use fewer convolution or fewer fully connected layers\n",
    "collect more data or further augment the data set\n",
    "Ideally, the model will make good predictions on both the training and validation sets. The implication is that when the network sees an image, it can successfully predict what angle was being driven at that moment.\n",
    "\n",
    "---\n",
    "\n",
    "- käytä osia tästä:\n",
    "For validation purposes 10% of the training data (about 1000 images) was held back. Only the center camera imags are used for validation. After few epochs (~10) the validation and training loss settle. The validation loss is consistently about half of the training loss, which indicates underfitting, however with the caveat that training and validation data are not drawn from the same sample: there is no data augmentation for the validation data. A more robust albeit non-automatic metric consists of checking the performance of the network by letting it drive the car on the second track which was not used in training.\n",
    "\n",
    "We used an Adam optimizer for training. All training was performed at the fastest graphics setting.\n",
    "\n",
    "####Generators and \"yield\"\n",
    "Mistä kyse: https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/\n",
    "- https://discussions.udacity.com/t/implementing-generator-help/230608/4\n",
    "\n",
    "\n",
    "\n",
    "#### Testing the Network\n",
    "\n",
    "https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/46a70500-493e-4057-a78e-b3075933709d/concepts/1ff2cbb5-2d9e-43ad-9424-4546f502fe20\n",
    "Set up your development environment with the CarND Starter Kit.\n",
    "\n",
    "Run the server.\n",
    "\n",
    "python drive.py model.h5\n",
    "\n",
    "If you're using Docker for this project: -> docker run -it --rm -p 4567:4567 -v pwd:/src udacity/carnd-term1-starter-kit python drive.py model.h5 or -> docker run -it --rm -p 4567:4567 -v ${pwd}:/src udacity/carnd-term1-starter-kit python drive.py model.h5.\n",
    "Port 4567 is used by the simulator to communicate.\n",
    "Once the model is up and running in drive.py, you should see the car move around (and hopefully not off) the track!\n",
    "If your model has low mean squared error on the training and validation sets but is driving off the track, this could be because of the data collection process. It's important to feed the network examples of good driving behavior so that the vehicle stays in the center and recovers when getting too close to the sides of the road.\n",
    "\n",
    "\n",
    "#### Conclusions\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprosessing steps (mitä tarkoittaa?)\n",
    "- normalized data\n",
    "- mean centered data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
